1. The first thing to do when working with data files for the first time, especially those created by someone else, is to look through them.
Do you notice any inconsistencies across files in how data are recorded?

`There are inconsistencies between all of the files on the amount of data points taken for MH and IH. For the file named Strdln_Twater_091015-100602100602_corrd.csv it is missing entire columns of data that the other files have.``

2. Let's say we are preparing to analyze these data in a program that has certain formatting requirements. For each of these requirements, write one or more series of Find and Replace statements using regex syntax.
  - The data fields are currently delimited by semicolons, but our program requires tab-delimited files.
  `sed -i"_backup" 's/;/-/g' Strdln_Twater_091015-100602_corrd.csv  for each file name`
  - Our program does not handle blank fields. Empty fields should be replaced with NaN (which stands for "Not a Number").
  `sed -i"backup" 's/\ /NaN/g' Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv`
    - Double check that you've done this properly. Do you still see empty fields? If so, why? Turn on invisible characters to help.
    `cat Strdln_Twater_091015-100602_corrd.csv`
  - Our program does not like whitespace on the end of lines. Remove all whitespace (spaces and tabs) from line ends.
  `sed -i"backup" 's/ *$//g' Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv`
  - Our program does not like spaces in any of its headers or fields. Replace spaces with underscores (_).
  `sed -i"backup" 's/\s/_/g' Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv`
  - Our program does not like lines with all missing information (only NaNs). Delete all characters from these lines.
  `sed -i"backup" 's/NaN/d/g' Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv`
  - Our program can only handle one digit after the decimal place for each number. Replace all numbers with trimmed versions.
`sed 's/\d+\.\d+/\d+\.\d{1}/g' Strdln_Twater_090611-090828_corrd.csv` Tried this but don't think it worked.

3. We also need one file that contains all measurements (from all input files) with samples from 2009. What is a series of Terminal commands (should only take 2 or 3) to create a new file with all 2009 measurements. This file should still have one header line at the top (and none elsewhere in the file). Save the commands you use to do
`cp Strdln_Twater_091015-100602_corrd.csv SampleData.csv for each file name`

4. Use regex and grep to extract all lines from all files that are complete (have no NaN values) and save them in a new file called `completeLakeTemps.txt`.
`grep -E \d Strdln_Twater_091015-100602_corrd.csv >> completeLakeTemps.txt for all files`

5. The full dataset is pretty large and we want to run some analyses that only have the measurements recorded on the 1st and 15th days of each month. Use grep with regex to create one combined file with just these measurements and save it as `SemiMonthlyLakeTemps.txt`.
`grep -E "d01" Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv >> SemiMonthlyLakeTemps.txt
grep -E "d15" Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv >> SemiMonthlyLakeTemps.txt`

6. We'd like to do an analysis that only looks at nighttime temperatures. Use grep and regex to create a file that contains samples taken between 8PM (20:00) and 6AM (06:00). Save this file as `NightTimeLakeTemps.txt`.
`grep -E "06:00" Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv >> NightTimeLakeTemps.txt  did this line for each hour-- inefficient but maybe it worked?`
7. Lastly, we'd like to do separate analyses for the temperatures taken at different depths. Create separate files that contain measurements for each depth (but across all lakes). Save these as `depth_0.1m.txt`, `depth_0.3m.txt`, etc.
`grep -E "7.0dm" Strdln_Twater_090611-090828_corrd.csv Strdln_Twater_090829-091012_corrd.csv Strdln_Twater_091015-100602_corrd.csv >> depth_7.0m.txt for each depth`
