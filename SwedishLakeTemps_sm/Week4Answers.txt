    1)The first thing to do when working with data files for the first time, especially those created by someone else, is to look through them. Do you notice any inconsistencies across files in how data are recorded?
       -No major differences were found, however, some cells have an empty field with missing data in it.

DB: What about columns that are present in one file, but not in the others?

    2)Let's say we are preparing to analyze these data in a program that has certain formatting requirements. For each of these requirements, write one or more series of Find and Replace statements using regex syntax.
    The data fields are currently delimited by semicolons, but our program requires tab-delimited files.
    Our program does not handle blank fields. Empty fields should be replaced with NaN (which stands for "Not a Number").
        Double check that you've done this properly. Do you still see empty fields? If so, why? Turn on invisible characters to help.
    Our program does not like whitespace on the end of lines. Remove all whitespace (spaces and tabs) from line ends.
    Our program does not like spaces in any of its headers or fields. Replace spaces with underscores (_).
    Our program does not like lines with all missing information (only NaNs). Delete all characters from these lines.
    Our program can only handle one digit after the decimal place for each number. Replace all numbers with trimmed versions.
        -In Sublime: Find /; Replace \t was done for tab-delimited files.
        
        DB: Good.
        
        -In Terminal: sed 's/\t/NaN/g' Strdln_Twater_090611-090828_corrd_sm.csv and this was done to all three files.
        
        DB: Tabs serve as the boundaries between fields. So, a blank cell would be \t\t and would need to be replaced with \tNaN\t.
        
        -In Sublime: Find '\s$' Replace $1 removed white spaces at the end of lines
        
        DB: Good, but what about multiple tabs or spaces? Might try "\s+$".
        
        -In Sublime: Find \s Replace _
        
        DB: I think you'll want to search a space " " specifically. Otherwise, you'll replace all the tabs that delimit your columns.
        
		-No lines were found to be completely missing data
	
		DB: There are a few. How might you find them?
	
        -In Sublime: Find (\d+\.)(\d{1})\d+ Replace $1$2 this was done to replace all numbers with trimmed versions.
        
        DB: Good! Could also write as (\d+\.\d)\d+ and replace as $1.

    3)We also need one file that contains all measurements (from all input files) with samples from 2009. What is a series of Terminal commands (should only take 2 or 3) to create a new file with all 2009 measurements. This file should still have one header line at the top (and none elsewhere in the file). Save the commands you use to do
         -grep-w'2009'--Strdln_Twater_090611-090828_corrd_sm.csv  Strdln_Twater_090829-091012_corrd_sm.csv  Strdln_Twater_091015-100602_corrd_sm.csv | uniq > 2009LakeData.csv

	DB: A few things to consider: what happens if you have other kinds of values that can also take the value 2009? Also, I think this will add the names of the original files to the beginning of each line.

    4)Use regex and grep to extract all lines from all files that are complete (have no NaN values) and save them in a new file called completeLakeTemps.txt.
	- grep -v "NaN" Strdln_Twater_090611-090828_corrd_sm.csv  Strdln_Twater_090829-091012_corrd_sm.csv  Strdln_Twater_091015-100602_corrd_sm.csv >> completeLakeTemps.csv

	DB: Good, just think about what to do about the header lines. We don't want these to occur in the middle of our file. Also, this may add filenames to each line in the output file.

    5)The full dataset is pretty large and we want to run some analyses that only have the measurements recorded on the 1st and 15th days of each month. Use grep with regex to create one combined file with just these measurements and save it as SemiMonthlyLakeTemps.txt.
	-curl 2009-[01-12]-[01-15] Strdln_Twater_090611-090828_corrd_sm.csv  Strdln_Twater_090829-091012_corrd_sm.csv  Strdln_Twater_091015-100602_corrd_sm.csv >> SemiMonthlyLakeTemps.txt

	DB: Good! But you mean "grep" and not "curl", right?

    6)We'd like to do an analysis that only looks at nighttime temperatures. Use grep and regex to create a file that contains samples taken between 8PM (20:00) and 6AM (06:00). Save this file as NightTimeLakeTemps.txt.
	-grep -E '([0-6]|2[0-3]):([0-5][0-9])'Strdln_Twater_090611-090828_corrd_sm.csv  Strdln_Twater_090829-091012_corrd_sm.csv  Strdln_Twater_091015-100602_corrd_sm.csv >> NightTimeLakeTemps.txt

	DB: I like this! But I think it might also capture times after 6:00, like 6:15, 6:30, etc.

    7)Lastly, we'd like to do a separate analysis for temperatures taken at one specific depth. Create a separate file that contains measurements for just the 0.1m depth in the lake MH. Save these as depth_0.1m.txt
	-awk '{print $3}' Strdln_Twater_090611-090828_corrd_sm.csv  Strdln_Twater_090829-091012_corrd_sm.csv  Strdln_Twater_091015-100602_corrd_sm.csv >> depth_0.1.txt
	
	DB: Good. We might want to also include the first column to keep the dates and times associated with the measurements.

